{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Homework \n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Wednesday May 2nd**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:42:13.096354Z",
     "start_time": "2018-04-20T02:42:12.091124Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 1: Building and Training a Feed-Forward Neural Network \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, `gradient_checking`, and a general `train` routine to learn the weights in your network via Stochastic Gradient Descent.  \n",
    "\n",
    "The skeleton for the `Network` class is below. Note that this class is almost identical to the one you worked with in the **Lecture 18** in-class notebook, so you should look there to remind yourself of the details.   Scroll down to find more information about your tasks as well as unit tests. \n",
    "\n",
    "**Important Note**: In **Problem 2** we'll be using the `Network` class to train a network to do handwritten digit recognition.  Please make sure to utilize vectorized Numpy routines as much as possible, as writing inefficient code here will cause very slow training times in **Problem 2**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:16:00.042109Z",
     "start_time": "2018-04-20T03:15:59.375181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "            \n",
    "        self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        \n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L-2, -1, -1):\n",
    "            pass\n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW  \n",
    "                    # TODO num_dW  \n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                    \n",
    "                # Check bias b[ell][ii]\n",
    "                \n",
    "                # TODO true_db  \n",
    "                # TODO num_db  \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                \n",
    "                # TODO: update weights and biases \n",
    "                self.W = self.W \n",
    "                self.b = self.b \n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%10)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `forward_prop` function in the `Network` class to implement forward propagation.  Your function should take in a single training example `x` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:46:58.761543Z",
     "start_time": "2018-04-20T02:46:58.749250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict` function in the `Network` class to take in a matrix of features and return a matrix of one-hot-encoded label predictions. Your one-hot-encoded predictions should correspond to the output neuron with the largest activation.   \n",
    "\n",
    "When you think your `predict` function is working well, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:01.982781Z",
     "start_time": "2018-04-20T02:47:01.974911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases. As in the **Lecture 18** in-class notebook, you may assume that the loss function for a single training example is given by \n",
    "\n",
    "$$\n",
    "C(y, {\\bf a}^L) = \\frac{1}{2}\\|y - {\\bf a}^L\\|^2  \n",
    "$$\n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:04.552132Z",
     "start_time": "2018-04-20T02:47:04.543799Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Due to the fact that we hard-code our various activation functions, cost functions, and their derivatives, it is vital to do some debugging to make sure we haven't made a mistake.  \n",
    "\n",
    "One common technique is to do **numerical gradient checking**.  In this method we compute numerical approximations of the derivatives of the cost function with respect to the model parameters and compare them to the analytic versions computed by back prop.  \n",
    "\n",
    "Consider a cost function $C$ which is a function of all of the weights and biases in the network.  We can estimate the derivative of $C$ with respect to a particular parameter using a numerical finite difference technique.  This process looks as follows \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i} \\approx \\frac{C(w_1,\\ldots, w_i+\\epsilon, \\ldots w_N) - C(w_1,\\ldots, w_i-\\epsilon, \\ldots w_N)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Evaluating the cost function with the perturbed weights can be accomplished by randomly choosing a training example, performing forward propagation, and then evaluating the cost function using the activations in the output layer.  \n",
    "\n",
    "I've given you starter code down below to do numerical gradient checking.  The code will compute the true and numerical values of the derivative of $C$ with respect to each parameter in the network and then plot the pairs of values as well as their relative errors.  Note that in practice this is extremely expensive, and we typically only check a few random parameters. \n",
    "\n",
    "When you believe your code is correct, you can test it by executing the following cell. Note that a good rule of thumb is to train the network for a handful of epochs before doing the gradient checking, to avoid any transient behavior that might occur at the very beginning of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call `back_prop` to get the derivatives, and then update the weights and the biases via SGD.  When you think you're done, execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Last but not least, we should implement $\\ell$-$2$ regularization.  Modify your `train` function to incorporate regularization of the weights (but **not** the biases) in your SGD update.  As in the Lecture 18 slides, you should assume that the cost function with regularization takes the form \n",
    "\n",
    "$$\n",
    "C_\\lambda = C + \\frac{\\lambda}{2} \\displaystyle\\sum_{w} w^2\n",
    "$$\n",
    "\n",
    "where $\\sum_{w}$ sums over each weight in all layers of the network. Think carefully before you go making large changes to your code.  This modification is much simpler than you think. When you think you're done, execute the following unit test.  (Then go back and execute the test in **Part C** to make sure you didn't break anything.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:07.378983Z",
     "start_time": "2018-04-20T02:47:07.368858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i nn_tests.py \"prob 1F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: A Neural Network Classifier for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll use the Feed-Forward Neural Network framework you wrote in **Problem 1** to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll again only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How do the true labels correspond to the associated one-hot-encoded label vectors? \n",
    "- Give an example of a network architecture with a single hidden layer that is compatible with this data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:07.591473Z",
     "start_time": "2018-04-20T03:43:07.485465Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"../data/mnist21x21_3789_one_hot.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:09.330860Z",
     "start_time": "2018-04-20T03:43:09.239140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCI2CIzQKjtD+D7QQyEKBqOIUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b6a4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)\n",
    "        \n",
    "training_index = 2\n",
    "label_dict = dict({0:3, 1:7, 2:8, 3:9})\n",
    "view_digit(X_train[training_index], label_dict[np.argmax(y_train[training_index])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Train a network with a single hidden layer containing $30$ neurons on the first $500$ training examples in the training set using a learning rate of $\\eta = 0.01$ for at least $50$ epochs.  What accuracy does your network achieve on the validation set?  Do you see any clear signs of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `Network` class so that it stores the accuracies on the training and validation data every $5$ epochs during the training process. Now increase the number of neurons in the hidden layer to $100$.  On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the learning rates $\\eta = 0.01$, $\\eta = 0.25$ and $\\eta = 1.5$.  Which learning rate seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**:  Now let's see if we can get better results with regularization. Using the best learning rate you found in **Part C**, on a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the regularization strengths $\\lambda = 10^{-6}$, $\\lambda = 10^{-4}$ and $\\lambda = 10^{-2}$.  Which regularization strength seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**:  Now let's see if we can get better results with different network architectures. On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the architecture from **Part D** as well as two other architectures.  Which architecture seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max 20 points] Extra Credit: Improving Network Performance \n",
    "***\n",
    "\n",
    "See if you can get better performance by exploring advanced techniques.  Things you might try are: \n",
    "\n",
    "- Implementing **Mini-Batch** Stochastic Gradient Descent \n",
    "- Experimenting with different activation functions (like tanh and **Leaky** ReLU)\n",
    "- Experimenting with different loss functions (like cross-entropy) \n",
    "\n",
    "For more detailed discussion of these techniques it'll be helpful to look at Chapter 3 of [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html). \n",
    "\n",
    "The amount of extra credit you receive will be proportional to the number of above suggested tasks that you complete.  Further, to receive credit for the tasks you must not only implement, but also provide evidence that you've tuned the network to make it work.  Comment on the performance differences between the original `Network` implementation and your new networks with bells and whistles. \n",
    "\n",
    "**Important Note**: Don't do any of these things in the original `Network` class, because you'll almost certainly break the unit tests.  Copy the `Network` class from above and rename it `BetterNetwork` (or something) and modify the new class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
