{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17: The Multilayer Perceptron\n",
    "***\n",
    "\n",
    "<img src=\"figs/nnbanner.png\" width=1100 height=50>\n",
    "\n",
    "**Reminder**: Scroll down and shift-enter the **Helper Functions** at the bottom of the notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Automatic Features and Learning XOR \n",
    "***\n",
    "\n",
    "\n",
    "Consider again the problem of learning $\\texttt{XOR}$ using a two-layer perceptron model. The training set is as follows: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccc}\n",
    "x_1 & 0 & 1 & 0 & 1 \\\\\n",
    "\\hline \n",
    "x_2 & 0 & 0 & 1 & 1 \\\\\n",
    "\\hline \n",
    "x_1 \\texttt{ XOR } x_2 & 0 & 1 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In the video lecture we found a two-layer perceptron with the following architecture \n",
    "\n",
    "<img src=\"figs/two_layer_perceptron.png\" width=400 height=50>\n",
    "\n",
    "and a set of weights and biases for the transition between each layer that accurately captured the $\\texttt{XOR}$ operator.  In this problem we consider slightly different weights, which also accurately capture $\\texttt{XOR}$. \n",
    "\n",
    "$$\n",
    "W^1 = \\left[\n",
    "\\begin{array}{rr}\n",
    "1 & 1 \\\\\n",
    "-1 & -1 \n",
    "\\end{array}\n",
    "\\right], \n",
    "\\quad\n",
    "{\\bf b}^1 = \\left[\n",
    "\\begin{array}{r}\n",
    "-0.5 \\\\\n",
    "1.5 \n",
    "\\end{array}\n",
    "\\right], \n",
    "\\quad\n",
    "W^2 = \\left[\n",
    "\\begin{array}{rr}\n",
    "1 & 1 \n",
    "\\end{array}\n",
    "\\right], \\quad\n",
    "{\\bf b}^2 = \\left[\n",
    "\\begin{array}{r}\n",
    "-1.5 \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "**Q**: One interpretation of the hidden layer activations is that they represent new features derived from the inputs that are learned automatically by the artificial network.  Compute the activations in the hidden layer for each training example and explain why these activations, interpreted as derived features, leads to a linearly separable set of points in feature space. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: A Multilayer Perceptron for Multiclass Classification\n",
    "***\n",
    "\n",
    "Consider finding a multilayer perceptron model to classify the following data into one of four classes: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|rrrrrrrrrr}\n",
    "x_1 & 1 & 2 & 0 & 0 & 0 & 1  & -1& -2&  \\\\\n",
    "\\hline \n",
    "x_2 & 0 & 1 & 1 & 2 & -1& -2 &  0& -1&  \\\\\n",
    "\\hline \n",
    "y & 0 & 0 & 1 & 1 & 2 &  2&  3 &  3&  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The data can be seen here where distinct colors represent the different class labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:16:26.322308Z",
     "start_time": "2018-04-02T19:16:26.147778Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,0],[2,1],[0,1],[0,2],[0,-1],[1,-2],[-1,0],[-2,-1]])\n",
    "y = np.array([0,0,1,1,2,2,3,3])\n",
    "prob2plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many nodes does the MLP have in the input layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many nodes does the MLP have in the output layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the minimal MLP architecture that you think can correctly classify this data set?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will determine (mostly through intuition) the weights and the biases necessary to classify this data set using the following MLP architecture:\n",
    "\n",
    "<img src=\"figs/MLP4ClassSmall.png\" width=400 height=50>\n",
    "\n",
    "**Q**: What are the dimensions of the weight matrices and biases $W^1$, ${\\bf b}^1$, $W^2$, ${\\bf b}^2$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: OK, now the fun part.  Determine weights and biases $W^1$, ${\\bf b}^1$, $W^2$, ${\\bf b}^2$ that will yield an MLP that correctly classifies the data set.  To help, I've given you some starter code and an MLP class that does the forward propagation and makes predictions about classes.  Before making any guesses, look through the code and make sure you understand how it works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:16:34.428428Z",
     "start_time": "2018-04-02T19:16:34.415972Z"
    }
   },
   "outputs": [],
   "source": [
    "W = []\n",
    "b = []\n",
    "W1 = np.array() #TODO \n",
    "W.append(W1)\n",
    "b1 = np.array() #TODO\n",
    "b.append(b1)\n",
    "W2 = np.array() #TODO \n",
    "W.append(W2)\n",
    "b2 = np.array() #TODO\n",
    "b.append(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:15:45.025608Z",
     "start_time": "2018-04-02T19:15:44.906055Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b \n",
    "        \n",
    "    def activation(self,z):\n",
    "        \"\"\"\n",
    "        Applies the element-wise indicator function I(z > 0)\n",
    "        \"\"\"\n",
    "        return (z>0).astype(int)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        \"\"\"\n",
    "        Uses the provided weights and biases to feed forward\n",
    "        examples in X and make predictions.  Returns numpy\n",
    "        array yhat \n",
    "        \"\"\"\n",
    "        yhat = np.zeros(X.shape[0])\n",
    "        for kk in range(X.shape[0]):\n",
    "            a = X[kk,:]\n",
    "            for (Wll, bll) in zip(W, b):\n",
    "                z = np.dot(Wll, a) + bll\n",
    "                a = self.activation(z)\n",
    "            yhat[kk] = np.argmax(a)\n",
    "        return yhat.astype(int) \n",
    "    \n",
    "    def plotDecisionBoundary(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Function to plot the decision boundary produced by \n",
    "        your MLP model.  X and y are optional data matrix \n",
    "        and label vector to overlay original data\n",
    "        \"\"\"\n",
    "        mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\", \"gray\": \"gray\"}\n",
    "        colorlist = [c for (n,c) in mycolors.items()]\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = fig.add_subplot(111) \n",
    "            \n",
    "        xline = np.linspace(-3,3,50) \n",
    "        yline = np.linspace(-3,3,50) \n",
    "        Xd = np.array([[xk, yk] for xk in xline for yk in yline])\n",
    "        \n",
    "        yhat = self.feedforward(Xd)\n",
    "        cd = [colorlist[yk] for yk in yhat]\n",
    "        plt.scatter(Xd[:,0], Xd[:,1], color=cd, s=25, alpha=0.5, zorder=2)\n",
    "        \n",
    "        if not(X is None or y is None): \n",
    "            colors = [colorlist[yk] for yk in y]\n",
    "            plt.scatter(X[:,0], X[:,1], color=colors, s=200, alpha=0.9)\n",
    "        \n",
    "        ax.xaxis.grid(True, color=\"gray\", ls='-', alpha=0.25)\n",
    "        ax.yaxis.grid(True, color=\"gray\", ls='-', alpha=0.25)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        plt.xticks([-3,-2,-1,0,1,2,3], fontsize=16)\n",
    "        plt.yticks([-3,-2,-1,0,1,2,3], fontsize=16)\n",
    "        ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "        plt.xlim([-3.5,3.5])\n",
    "        plt.ylim([-3.5,3.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:15:46.874836Z",
     "start_time": "2018-04-02T19:15:46.869325Z"
    }
   },
   "outputs": [],
   "source": [
    "myMLP = MLP(W, b)\n",
    "yhat = myMLP.feedforward(X)\n",
    "print(\"yhat = \", yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Once you're satisfied that your MLP is correctly classifying the 8 data points, call the $\\texttt{plotDecisionBoundary}$ method to print a rough estimate of the decision boundary of your classifier.  Does it look like you expect?  Why or why not?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:15:48.653582Z",
     "start_time": "2018-04-02T19:15:48.254291Z"
    }
   },
   "outputs": [],
   "source": [
    "myMLP.plotDecisionBoundary(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: TensorFlow Playground \n",
    "***\n",
    "\n",
    "Go <a href=\"http://playground.tensorflow.org/\"> here</a> and fiddle with things! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T19:14:20.974831Z",
     "start_time": "2018-04-02T19:14:20.773697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def prob2plot(X, y):\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\", \"gray\": \"gray\"}\n",
    "    colorlist = [c for (n,c) in mycolors.items()]\n",
    "    colors = [colorlist[yk] for yk in y ]\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111) \n",
    "    plt.scatter(X[:,0], X[:,1], color=colors, s=200, alpha=0.9, zorder=2)\n",
    "    ax.xaxis.grid(True, color=\"gray\", ls='-', alpha=0.25)\n",
    "    ax.yaxis.grid(True, color=\"gray\", ls='-', alpha=0.25)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([-3,-2,-1,0,1,2,3], fontsize=16)\n",
    "    plt.yticks([-3,-2,-1,0,1,2,3], fontsize=16)\n",
    "    ax.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    plt.xlim([-3.5,3.5])\n",
    "    plt.ylim([-3.5,3.5])\n",
    "    \n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
